[IO_INFORMATION]
#Training dir, models are here under /models folder
train_dir = . 
# train data dir 
data_dir = 
# validation data dir
val_data_dir = 
#If True, load the latest weights, otherwise starts training from scratch
load_weights = True
# restarting from a specific step
#weights_file = models/epoch_2110_step_2110000.pkl
# save the model every n epoch
save_epoch_freq = 50
# set True for acceleration in training, small batches
data_cache = True

[DATA_INFORMATION]
atomic_sequence = C
input_format = example   
output_offset = -5.5

[MODEL_INFORMATION]
## Our standard model
model_type = MLP
## Architecture
architecture = 128:64:1
descriptor_type = LATTE
nn_mode = list
pair_mod = 500
at_mod = 50
## Cutoff radius
Rc = 5.5
sig = 1.0
descriptor_shape = 100,-:200,i,i:200,ij,ij:200,i,j,ij

[TRAINING_PARAMETERS]
#Batch size, too big might run out of memory
batch_size = 8
#Learning rate: you can concatenate schedules with a comma, this is just an example:
#const:lr:st means use learning rate lr for st number of steps
#exp:lr:dec:st means start from lr and decay by dec in st steps.
learning_rate = const:1e-3:100000, exp:1e-3:1e-1:100000, const:1e-4:3000000
#L1 regularization
l1_coef = 0.001
#Total number of "epochs"
max_epochs = 6000000
#How many steps make an epoch
steps_per_epoch = 1000
#Forces cost in loss func
forces_cost = 1.0
#Energy cost in loss func
energy_cost = 10.0
#This uses the energy square loss per atom in the loss function
energy_loss = per_at
